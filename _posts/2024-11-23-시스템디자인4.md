---
title : 광고 클릭 이벤트 집계
date: 2024-11-23 +1800
categories: [System Design]
tags: [interview-prep]
---
# 광고 클릭 이벤트 집계
디지털 광고의 핵심 프로세스는 RTB (Real-time Bidding), 즉 실시간 경매
- 경매를 통해 광고가 나갈 지면을 거래

![img](/assets/img/sysdesign/chapter6/RTB.png)

- 속도가 매우 중요! (보통 1초 내에 모든 프로세스 마무리)
- 데이터의 정확성 중요! 
  - 집계로 온라인 광고가 얼마나 효율적이었는지 측정
  - 광고주가 얼마나 많은 돈을 지불할지 영향
  - 집계된 광고 클릭 데이터 기반 지표
    - Click Through Rate
    - Conversion Rate

## 1단계 : 문제 이해 및 설계 범위 확장
### 기능 요구사항
- 지난 M분 동안의 ad_id 클릭 수 집계
- 매분 가장 많이 클릭된 상위 100개 광고 아이디를 반환
- 다양한 속성에 따른 집계 필터링을 지원
- 데이터의 양은 페이스북이나 구글 규모

### 비기능 요구사항
- 집계 결과 정확성은 데이터가 RTB 및 광고 과금에 사용되므로 중요
- 지연되거나 중복된 이벤트를 적절히 처리할 수 있어야함
- 견고성 (reliability) 부분적인 장애는 감내할 수 있어야 함
- 지연 시간 요구사항, 전체 처리 시간은 최대 수 분을 넘지 않아야 함

### 개략적 추정
- 일간 능동 사용자 (DAU) 수는 10억명 (1billion)
- 각 사용자는 하루에 평균 1개 광고를 클릭한다고 가정. 따라서 하루에 10억건의 광고 클릭 이벤트가 발생
- 광고 클릭 QPS = $\frac{10^9 event}{하루 10^5초}$ = 10,000
- 최대 광고 클릭 QPS는 평균 QPS의 다섯배, 50,000 QPS로 가정
- 광고 클릭 이벤트 하나당 0.1KB의 저장 용량이 필요하다고 가정
  - 일일 저장소 요구량은 0.1KB * 10억 = 100GB
  - 월간 저장 용량 요구량은 대략 3TB

## 2 단계 : 개략적 설계안 제시 및 동의 구하기
### 질의 API 설계
목적 : 클라이언트와 서버간의 통신 규약
- 클라이언트 : 데이터 과학자, 제품관리자, 광고주
  - 이들이 대시보드를 이용하는 순간 집계 서비스에 질의가 발생

기능 요구 사항
- 지난 M분동안 각 ad_id에 발생한 클릭수 집계
- 지난 M분동안 가장 많이 클릭이 발생한 상위 N개 ad_id 목록 반환
- 다양한 속성을 기준으로 집계 결과를 필터링하는 기능 지원

**API1 : 지난 M분간 각 ad_id에 발생한 클릭수 집계**

```GET /v1/ads/{:ad_id}/aggregated_count```
| Parameter | Response |
| --------- | -------- |
| from      | ad_id    |
| to        | count    |
| filter    |          |


**API2 : 지난 M분간 가장 많은 클릭이 발생한 상위 N개 ad_id목록**

```GET /v1/ads/popular_ads```
| Parameter | Response |
| --------- | -------- |
| count     | ad_ids   |
| window    |          |
| filter    |          |

### 데이터 모델
이 시스템이 다루는 데이터는 두 종류로 나눌 수 있음
 - 원시 raw data
 - 집계 결과 aggregated data

**Raw Data**
|ad_id  	|click_timestamp 	|user_id  	|ip  	|country|
|---	|---	|---	|---	|---    |
|ad001  |2021-01-01 00:00:01  |user1  |207.148.22.22  |USA   |

**Aggregated Data**
|window_size  	|updated_time_minute  	|most_clicked_ads  	|
|---	|---	|---	|
|  	|  	|  	|

**비교**
|  	|원시 데이터만 보관  	|집계 결과 데이터만 보관  	|
|---	|--- 	|---	|
|장점  	|- 원본데이터를 손실 없이 보관 <br>- 데이터 필터링 및 재계산 지원  	|- 데이터 용량 절감<br>- 빠른 질의 성능  	|
|단점  	|- 막대한 데이터 용량<br>-낮은 질의 성능  	|- 데이터 손실.원본 데이터 아닌 계산/유도된 데이터를 저장하는 데서 오는 결과. <br> 예) 10개의 원본 데이터는 1개의 데이터로 집계/축약될 수 있다.  	|

> 둘 다 저장필요!
> - 디버깅 용도를 위해 원시 데이터 보관
> - 직접 질의를 위해 집계 결과 데이터
> - 백업을 위해 원시 데이터
>   - 오래된 원시 데이터는 cold storage 로
> - 활성 데이터를 위해 집계 결과
>   - 질의 성능을 높이기 위해 튜닝 필요

### 데이터 베이스 선택

평가 요소
- 데이터의 모습, 문서? 이진 대형 객체?
- 작업 흐름이 읽기 중심? 쓰기 중심? 둘 다?
- 트랜잭션 지원?
- sum, count 같은 분석 처리 함수를 많이 써야 하는지?

원시 데이터
- 일상적인 작업을 위해선 질의필요 x
- 평균 쓰기 QPS는 10,000 이고 최대 50,000
- 쓰기 중심 시스템
- 원시 데이터는 백업과 재계산 용도로만 이용되므로 이론적으로 읽기 연산 빈도는 낮다. 

=> 카산드라

집계 데이터
- 읽기 연산 + 쓰기 연산 둘 다 많이

```원시 + 집계 데이터 같은 유형의 데이터 베이스 활용 가능```

### 개략적 설계안
![img](/assets/img/sysdesign/chapter6/AggWorkFlow.png)

- 입력은 원시데이터 (무제한 데이터 스트림), 출력은 집계 결과

**비동기 처리**
- 위 방법은 데이터를 동기식으로 처리
  - 트래픽이 갑자기 증가하면 메모리 부족 오류 등 문제
- 해결방안 : 카프카 같은 메세지 큐를 도입하여 생산자와 소비자의 결합을 끊는 것

> 대략적 설계안
> - 로그 감시자
> - 집계 서비스
> - 데이터베이스
>   - 두개의 메시지 큐
>   - 기록 프로세스는 메시지 큐에서 데이터를 꺼내 데이터베이스가 지원하는 형식으로 변환한 다음 기록하는 역활

![img](/assets/img/sysdesign/chapter6/atomic-commit.png)

- 정확하게 한 번 데이터를 처리하기 위해 카프카 같은 시스템을 두번째 메시지 큐로 도입
  - 일반적인 메시지 큐의 경우
    - 메시지 중복 처리 가능성
      - 소비자가 메시지를 처리했지만 처리 완료를 보내기 전에 연결이 끊김
      - 메시지는 큐에 다시 남아 있거나 재전송될 수 있음
      - "동일한 메시지가 여러번 처리될 가능성이 생김"
    - 데이터 손실 가능성
      - 메세지가 성공적으로 처리되었다고 잘못 ACK를 보냈지만 실제 처리 결과는 손실되었을 경우, 메시지가 완전히 사라질 수 있음
  - 카프카의 경우
    - 메시지의 내구성
      - 디스크의 저장하여 데이터 손실 가능성을 줄임
      - 브로커에 장애가 발생해도 복제본을 통해 메시지 복구 가능
    - offset 기반 처리
      - 소비자가 읽은 메시지의 offset을 관리함 (중복 처리 방지)
    - EOS (Exactly-once Semantics)
      - 트랜잭션 기능을 통해 정확히 한 번 데이터를 처리하도록 보장
      - 생산자가 데이터를 여러 파티션에 쓰거나, 소비자가 데이터를 처리한 후 다른 주제로 데이터를 전송할 때 트랜잭션으로 묶어 일관된 상태를 유지할 수 있음
    - Replay 가능성
      - 메시지를 삭제하지 않고 설정된 기간 동안 유지
      - offset을 조정하여 이전 메시지를 다시 읽을 수 있음

**집계 서비스**
- MapReduce, DAG
  - 시스템을 작은 컴퓨팅 단위로 세분화
  - 각 노드는 한가지 작업만 처리하며, 처리 결과를 다음 노드에 인계

꼭 맵노드가 필수일까?
- 데이터가 생성되는 방식에 제어권이 없을 경우 서로 동일한 id 를 갖는 이벤트가 다른 카프카 파티션에 입력될 수도 있다. 

## 3단계 : 상세 설계
- 스트리밍vs일괄처리
  - 카파 아키텍처 : 일괄처리와 스트리밍 처리 경로를 하나로 결합
  - 핵심 : 단일 스트림 처리 엔진을 사용하여 실시간 데이터 처리 및 끊임없는 데이터 재처리 문제를 해결
  - 데이터 재계산
    - 전용 집계 서비스를 두어 실시간 데이터 처리과정이 과거 데이터 재처리 프로세스와 간섭하는 일을 막음
    - ![img](/assets/img/sysdesign/chapter6/recalc.png)
- 시간과 집계윈도(aggregation window) 
  - 네트워크 지연 비동기적 처리환경 때문에 이벤트가 발생한 시각과 처리 시각 사이에 격차가 커 질 수 있음
    - 워터마크 기법은 데이터의 정확도는 높아지지만 대기시간이 늘어나 전반적인 지연시간이 는다는 단점이 있음
    - 텀블링 윈도를 사용하여 매분 발생한 클릭 이벤트를 집계하기 아주 좋고
    - 슬라이딩 윈도로 M분간 가장 많이 클릭된 상위 N개 광고를 알아내기에 적합
- 전달보증(delivery guarantee)
  - 데이터의 정확성과 무결성이 아주 중요
  - 따라서, "정확히 한 번" 방식을 권장
  - 데이터의 중복 제거
    - offset 기록하기 -> what if 집계 서비스 노드에 장애가 발생하면? -> 다운스트림에서 집계 결과 수신 확인 응답을 받은 후 offset을 저장하자
      - 이벤트를 정확하게 한번만 처리하려면 분산 트랜잭션에 넣어야함
      - 여러 노드에서 작동하는 트랜잭션으로 그 안에서 실행하는 작업 가운데 하나라도 실패하면 모든 작업의 상태를 실행 전으로 되돌리게 됨
      - ![img](/assets/img/sysdesign/chapter6/disttrans.png)
- 시스템의규모확장
  - 메시지 큐, 집계서버, 데이터베이스 - 3가지의 독립 구성 요소 (결합도가 낮아 독립적으로 규모를 늘릴 수 있음)
    - 메시지 큐 : 소비자 그룹 내의 재조정
    - 집계 서비스 : ad_id마다 별도의 처리 스레드
    - 데이터 베이스 : 카산드라 (안정해시와 유사한 방식으로) 수평적인 규모확장을 기본적 지원
      - 핫스팟은 추가 집계 서비스 노드를 할당해 처리
- 데이터모니터링 및 정확성
  - 시스템의 중요 부분마다 시각 추정이 가능하도록 만들어야함
    - 카프카의 경우 레코드 처리 지연 지표를 추적
  - 조정
    - 파티션에 기록된 클릭이벤트를 이벤트 발생 시각에 따라 정렬한 결과를 일괄 처리하여 만들어 낸 다음, 실시간 집계 결과와 비교해 보는 것
- 결함내성(fault tolerance)
  - "시스템 상태"에 해당하는 정보 (offset) 말고도 지난 M분간 가장 많이 클릭된 광고 N개 같은 데이터도 시스템 상태의 일부로 저장
  - 스냅숏 이용
    - 어떤 노드에 장애가 발생하면 해당 노드를 새것으로 대체한 다음 마지막 스냅숏에서 데이터를 복구
    - 스냅숏을 마지막으로 찍은후에 도착한 새로운 이벤트는 새 집계 서비스 노드가 카프카 브로커에서 읽어가 다시 처리
- 최종 설계 다이어그램
  - ![img](/assets/img/sysdesign/chapter6/final-arc.png)